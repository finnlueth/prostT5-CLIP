{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-j8s_jbk8 because the default path (/home/lfi/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, T5Tokenizer, AutoModelForCausalLM, T5EncoderModel\n",
    "\n",
    "from src._shared import load_config\n",
    "from src.model.modeling_protein_clip import smart_mean_pooling, attention_mask_to_trim_indices\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = load_config()\n",
    "\n",
    "tokenizer_llm = AutoTokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path=train_config[\"model\"][\"text_encoder_name\"],\n",
    "        )\n",
    "tokenizer_plm = T5Tokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path=train_config[\"model\"][\"protein_encoder_name\"],\n",
    "            do_lower_case=False,\n",
    "            use_fast=True,\n",
    "            legacy=False,\n",
    "        )\n",
    "\n",
    "dummy_texts = [\"This is a test protein sequence text\", \"This is a different protein test sequence\"]\n",
    "dummy_proteins = [\n",
    "    \"MLKFVVVLAAVLSLYAYAPAFEVHNKKNVLMQRVGETLRISDRYLYQTLSKPYKVTLKTLDGHEIFEVVGEAPVTFRFKDKERPVVVASPEHVVGIVAVHNGKIYARNLYIQNISIVSAGGQHSYSGLSWRYNQPNDGKVTDYF\",\n",
    "    \"MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSELDKAIGRNTNGVITKDEAEKLFNQDVDAAVRGILRNAKLKPVYDSLDAVRRAALINMVFQMGE\",\n",
    "]\n",
    "dummy_proteins = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", x))) for x in dummy_proteins]\n",
    "\n",
    "text_tokens = tokenizer_llm(dummy_texts, return_tensors=\"pt\", padding=True, truncation=False)\n",
    "protein_tokens = tokenizer_plm(dummy_proteins, return_tensors=\"pt\", padding=True, truncation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  910,   338,   263,  1243, 26823,  5665,  1426],\n",
      "        [  910,   338,   263,  1422, 26823,  1243,  5665]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1]])}\n",
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "tensor([  910,   338,   263,  1243, 26823,  5665,  1426])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1])\n",
      "This is a test protein sequence text\n"
     ]
    }
   ],
   "source": [
    "print(text_tokens)\n",
    "print(text_tokens.keys())\n",
    "print(text_tokens[\"input_ids\"][0])\n",
    "print(text_tokens[\"attention_mask\"][0])\n",
    "print(tokenizer_llm.decode(text_tokens[\"input_ids\"][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 4 14 15 6 6 6 4 3 3 6 4 7 4 18 3 18 3 13 3 15 9 6 20 17 14 14 17 6 4 19 16 8 6 5 9 11 4 8 12 7 10 8 18 4 18 16 11 4 7 14 13 18 14 6 11 4 14 11 4 10 5 20 9 12 15 9 6 6 5 9 3 13 6 11 15 8 15 14 10 14 9 8 13 6 6 6 3 7 13 9 20 6 6 5 12 6 3 6 20 17 5 14 12 18 3 8 17 4 18 12 16 17 12 7 12 6 7 3 5 5 16 20 7 18 7 5 4 7 21 8 18 17 16 13 17 10 5 14 6 11 10 18 15 1\n",
      "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "M L K F V V V L A A V L S L Y A Y A P A F E V H N K K N V L M Q R V G E T L R I S D R Y L Y Q T L S K P Y K V T L K T L D G H E I F E V V G E A P V T F R F K D K E R P V V V A S P E H V V G I V A V H N G K I Y A R N L Y I Q N I S I V S A G G Q H S Y S G L S W R Y N Q P N D G K V T D Y F</s>\n",
      "19 17 12 15 9 19 4 8 12 10 9 5 4 8 4 14 12 18 14 10 11 9 5 18 18 11 12 5 12 5 20 4 4 11 14 7 13 7 4 17 3 3 14 7 9 4 10 14 3 12 5 8 17 11 17 5 6 12 11 14 10 9 3 9 14 4 15 17 16 10 6 10 3 3 6 8 5 12 4 8 17 3 14 4 14 13 6 18 10 7 4 10 3 6 8 8 3 3 4 12 17 19 6 15 16 19 5 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "M N I F E M L R I D E G L R L K I Y K D T E G Y Y T I G I G H L L T K S P S L N A A K S E L D K A I G R N T N G V I T K D E A E K L F N Q D V D A A V R G I L R N A K L K P V Y D S L D A V R R A A L I N M V F Q M G E</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# print(protein_tokens)\n",
    "# print(protein_tokens.keys())\n",
    "print(*protein_tokens[\"input_ids\"][0].tolist())\n",
    "print(*protein_tokens[\"attention_mask\"][0].tolist())\n",
    "print(tokenizer_plm.decode(protein_tokens[\"input_ids\"][0]))\n",
    "print(*protein_tokens[\"input_ids\"][1].tolist())\n",
    "print(*protein_tokens[\"attention_mask\"][1].tolist())\n",
    "print(tokenizer_plm.decode(protein_tokens[\"input_ids\"][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 145])\n",
      "torch.Size([2, 145])\n"
     ]
    }
   ],
   "source": [
    "print(protein_tokens[\"input_ids\"].shape)\n",
    "print(protein_tokens[\"attention_mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = {k: v.to('cuda') for k, v in text_tokens.items()}\n",
    "protein_tokens = {k: v.to('cuda') for k, v in protein_tokens.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5EncoderModel(\n",
       "  (shared): Embedding(128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_plm, loading_info_plm = T5EncoderModel.from_pretrained(\n",
    "    pretrained_model_name_or_path='Rostlab/prot_t5_xl_uniref50',\n",
    "    device_map='cuda:0',\n",
    "    output_loading_info=True,\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model_plm.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_plm.eval()\n",
    "with torch.no_grad():\n",
    "    model_output = model_plm(protein_tokens[\"input_ids\"], protein_tokens[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 145, 1024])\n",
      "tensor([[[ 0.2427, -0.2669,  0.2241,  ...,  0.4926, -0.0906,  0.0016],\n",
      "         [-0.0530, -0.1876,  0.1500,  ...,  0.2387, -0.0264, -0.0929],\n",
      "         [ 0.0649, -0.0347,  0.3434,  ...,  0.1018, -0.0669, -0.1747],\n",
      "         ...,\n",
      "         [ 0.0078,  0.1548, -0.0381,  ..., -0.0581, -0.0658, -0.3015],\n",
      "         [-0.2471, -0.0544,  0.0465,  ...,  0.1646,  0.0707, -0.3671],\n",
      "         [ 0.0017, -0.0949,  0.0099,  ..., -0.0407, -0.0417,  0.0333]],\n",
      "\n",
      "        [[-0.0464, -0.3680,  0.2976,  ..., -0.0458, -0.0230, -0.0038],\n",
      "         [-0.2348, -0.1525,  0.1276,  ...,  0.0795,  0.1079, -0.0907],\n",
      "         [ 0.0544,  0.1019,  0.2195,  ...,  0.1389, -0.0771, -0.1166],\n",
      "         ...,\n",
      "         [-0.0218, -0.2385,  0.1239,  ..., -0.1804, -0.2499,  0.0594],\n",
      "         [-0.0318, -0.2399,  0.1235,  ..., -0.1831, -0.2554,  0.0663],\n",
      "         [ 0.0308, -0.2422,  0.0607,  ..., -0.1697, -0.2450, -0.0066]]],\n",
      "       device='cuda:0')\n",
      "\n",
      "torch.Size([2, 1024])\n",
      "tensor([[ 0.0150,  0.0740,  0.0449,  ...,  0.0753,  0.0156, -0.0082],\n",
      "        [ 0.0112, -0.0887,  0.0909,  ..., -0.0619, -0.0747, -0.0139]],\n",
      "       device='cuda:0')\n",
      "\n",
      "torch.Size([2])\n",
      "tensor([ 9.1497e-06, -8.4553e-04], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model_output['last_hidden_state'].shape)\n",
    "print(model_output['last_hidden_state'])\n",
    "print()\n",
    "print(model_output['last_hidden_state'].mean(dim=1).shape)\n",
    "print(model_output['last_hidden_state'].mean(dim=1))\n",
    "print()\n",
    "print(model_output['last_hidden_state'].mean(dim=1).mean(dim=-1).shape)\n",
    "print(model_output['last_hidden_state'].mean(dim=1).mean(dim=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1024])\n",
      "tensor([[ 0.0150,  0.0740,  0.0449,  ...,  0.0753,  0.0156, -0.0082],\n",
      "        [ 0.0223, -0.0566,  0.0859,  ..., -0.0246, -0.0437, -0.0225]],\n",
      "       device='cuda:0')\n",
      "tensor([9.1497e-06, 1.4323e-03], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "pooled_output = smart_mean_pooling(model_output['last_hidden_state'], protein_tokens[\"attention_mask\"])\n",
    "print(pooled_output.shape)\n",
    "print(pooled_output)\n",
    "print(pooled_output.mean(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1024])\n",
      "tensor([[ 0.0151,  0.0752,  0.0451,  ...,  0.0761,  0.0160, -0.0085],\n",
      "        [ 0.0227, -0.0564,  0.0874,  ..., -0.0242, -0.0443, -0.0229]],\n",
      "       device='cuda:0')\n",
      "tensor([4.9078e-06, 1.4458e-03], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "pooled_output = smart_mean_pooling(model_output['last_hidden_state'], attention_mask_to_trim_indices(protein_tokens[\"attention_mask\"], trim_end=1))\n",
    "print(pooled_output.shape)\n",
    "print(pooled_output)\n",
    "print(pooled_output.mean(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0],\n",
      "        [1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0]])\n",
      "\n",
      "tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask_to_trim_indices(torch.tensor([[0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0], [1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0]])))\n",
    "print()\n",
    "print(attention_mask_to_trim_indices(torch.tensor([[0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0], [1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0]]), trim_beginning=2, trim_end=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
