{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtuNusLh7x1f",
        "outputId": "43c7ebb1-66ff-48cb-c491-8abb40d70a6f"
      },
      "outputs": [],
      "source": [
        "!pip install flash-attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbQCr7_C7h0W",
        "outputId": "03024c0b-0986-4770-e01f-139a5a835785"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import h5py\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.random.manual_seed(0)\n",
        "\n",
        "# Create a small dataset\n",
        "data = [\n",
        "    {\"uid\": \"A001\", \"seq\": \"MLEVPVWIPILAFAVGLGLGLLIPHLQKPFQRF\", \"text\": \"This protein is involved in membrane transport.\"},\n",
        "    {\"uid\": \"A002\", \"seq\": \"MSLEQKKGADIISKILQIQNSIGKTTSPSTLKT\", \"text\": \"This enzyme catalyzes the hydrolysis of ATP.\"},\n",
        "    {\"uid\": \"A003\", \"seq\": \"MKMKQQGLVADLLPNIRVMKTFGHFVFNYYNDN\", \"text\": \"This transcription factor regulates gene expression.\"}\n",
        "]\n",
        "\n",
        "# Save the dataset as a CSV file\n",
        "csv_file = 'sample_data.csv'\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv(csv_file, index=False)\n",
        "print(f\"Sample data saved to {csv_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1RNp6Sr_URq"
      },
      "outputs": [],
      "source": [
        "# Load the model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3.5-mini-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eXCO6nb-8qRu"
      },
      "outputs": [],
      "source": [
        "# Function to extract embeddings\n",
        "def extract_embeddings(text, sentence_level=True):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "    last_hidden_state = outputs.hidden_states[-1]\n",
        "\n",
        "    if sentence_level:\n",
        "        # Average over tokens to get sentence embedding\n",
        "        embeddings = last_hidden_state.mean(dim=1)\n",
        "    else:\n",
        "        # Keep per-token embeddings\n",
        "        embeddings = last_hidden_state.squeeze(0)\n",
        "\n",
        "    # Convert to float32 before converting to numpy\n",
        "    return embeddings.cpu().float().numpy()\n",
        "\n",
        "# Function to process CSV and save embeddings\n",
        "def process_csv_to_hdf5(csv_file, hdf5_file, sentence_level=True):\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Create an HDF5 file\n",
        "    with h5py.File(hdf5_file, 'w') as f:\n",
        "        # Create a group for sentence-level or token-level embeddings\n",
        "        group_name = 'sentence_embeddings' if sentence_level else 'token_embeddings'\n",
        "        group = f.create_group(group_name)\n",
        "\n",
        "        # Process each row in the DataFrame\n",
        "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing entries\"):\n",
        "            uid = row['uid']\n",
        "            text = row['text']\n",
        "\n",
        "            # Extract embeddings\n",
        "            embedding = extract_embeddings(text, sentence_level)\n",
        "\n",
        "            # Save embeddings to HDF5 file\n",
        "            group.create_dataset(uid, data=embedding)\n",
        "\n",
        "    print(f\"Embeddings saved to {hdf5_file}\")\n",
        "\n",
        "    # Verify the contents of the HDF5 file\n",
        "    with h5py.File(hdf5_file, 'r') as f:\n",
        "        print(f\"\\nContents of the HDF5 file ({group_name}):\")\n",
        "        for key in f[group_name].keys():\n",
        "            print(f\"UID: {key}, Shape: {f[group_name][key].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkXuqBwB_lrB",
        "outputId": "32c127d1-50a6-43c0-9d09-dbf307c115d9"
      },
      "outputs": [],
      "source": [
        "# Save sentence-level embeddings\n",
        "process_csv_to_hdf5(csv_file, 'sentence_embeddings.h5', sentence_level=True)\n",
        "\n",
        "# Save token-level embeddings\n",
        "process_csv_to_hdf5(csv_file, 'token_embeddings.h5', sentence_level=False)\n",
        "\n",
        "# Clean up the CSV file\n",
        "os.remove(csv_file)\n",
        "print(f\"\\nRemoved temporary CSV file: {csv_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4zhot5u_hld"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "csv_file = 'your_file.csv'  # Datafile with `uid` and `text` columns\n",
        "hdf5_file = 'embeddings.h5'  # output path\n",
        "\n",
        "# Save sentence-level embeddings\n",
        "process_csv_to_hdf5(csv_file, 'sentence_embeddings.h5', sentence_level=True)\n",
        "\n",
        "# Save token-level embeddings\n",
        "process_csv_to_hdf5(csv_file, 'token_embeddings.h5', sentence_level=False)a"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
