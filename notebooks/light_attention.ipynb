{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "from Bio import SeqIO\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, recall_score\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedModel,\n",
    "    T5EncoderModel,\n",
    "    T5Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    modeling_outputs,\n",
    ")\n",
    "from transformers.utils import (\n",
    "    is_datasets_available,\n",
    ")\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = {\n",
    "    \"project_name\": \"light_attention_localization\",\n",
    "    \"seed\": 42,\n",
    "    \"weights_and_biases\": {\n",
    "        \"enabled\": True,\n",
    "        \"project_name\": \"light_attention_localization\",\n",
    "        \"report_to\": \"wandb\",\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"plm\": \"Rostlab/prot_t5_xl_uniref50\",\n",
    "        \"freeze_plm\": True,\n",
    "    },\n",
    "    \"light_attention\": {\n",
    "        \"dropout\": 0.25,\n",
    "        \"kernel_size\": 9,\n",
    "        \"output_dim\": 10,\n",
    "    },\n",
    "    \"lora\": {\n",
    "        \"enabled\": False,\n",
    "        \"r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"use_rslora\": False,\n",
    "        \"use_dora\": False,\n",
    "        \"target_modules\": [],\n",
    "        \"modules_to_save\": [],\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"train_batch_size\": 64,\n",
    "        \"num_epochs\": 1,\n",
    "        \"eval_batch_size\": 32,\n",
    "        \"eval_strategy\": \"steps\",\n",
    "        \"eval_steps\": 64,\n",
    "        \"eval_on_start\": True,\n",
    "        \"eval_sample_size\": 32,\n",
    "        \"batch_eval_metrics\": True,\n",
    "        \"remove_unused_columns\": False,\n",
    "        \"save_strategy\": \"no\",\n",
    "        \"save_steps\": 300,\n",
    "        \"save_total_limit\": 5,\n",
    "        \"logging_steps\": 1,\n",
    "        \"lr_scheduler_type\": \"cosine\",\n",
    "        \"warmup_steps\": 100,\n",
    "    },\n",
    "}\n",
    "\n",
    "model_name_identifier = train_config[\"project_name\"] + \"-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "label2location = {\n",
    "    \"Cell.membrane\": 0,\n",
    "    \"Cytoplasm\": 1,\n",
    "    \"Endoplasmic.reticulum\": 2,\n",
    "    \"Extracellular\": 3,\n",
    "    \"Golgi.apparatus\": 4,\n",
    "    \"Lysosome/Vacuole\": 5,\n",
    "    \"Mitochondrion\": 6,\n",
    "    \"Nucleus\": 7,\n",
    "    \"Peroxisome\": 8,\n",
    "    \"Plastid\": 9,\n",
    "}\n",
    "\n",
    "location2label = {\n",
    "    0: \"Cell.membrane\",\n",
    "    1: \"Cytoplasm\",\n",
    "    2: \"Endoplasmic.reticulum\",\n",
    "    3: \"Extracellular\",\n",
    "    4: \"Golgi.apparatus\",\n",
    "    5: \"Lysosome/Vacuole\",\n",
    "    6: \"Mitochondrion\",\n",
    "    7: \"Nucleus\",\n",
    "    8: \"Peroxisome\",\n",
    "    9: \"Plastid\",\n",
    "}\n",
    "\n",
    "print(model_name_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_config[\"weights_and_biases\"][\"enabled\"]:\n",
    "    import wandb\n",
    "    run = wandb.init(project=train_config[\"weights_and_biases\"][\"project_name\"], name=model_name_identifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"Rostlab/prot_t5_xl_uniref50\",\n",
    "    do_lower_case=False,\n",
    "    use_fast=True,\n",
    "    legacy=False,\n",
    ")\n",
    "\n",
    "if not os.path.exists(\"../tmp/data/unprocessed/localization/deeploc_data_processed\"):\n",
    "    train_sequences = []\n",
    "    train_ids = []\n",
    "    train_labels = []\n",
    "    with open(\"../tmp/data/unprocessed/localization/deeploc_data.fasta\", \"r\") as f:\n",
    "        for record in SeqIO.parse(f, \"fasta\"):\n",
    "            train_sequences.append(str(record.seq))\n",
    "            desc_parts = record.description.split()\n",
    "            train_ids.append(desc_parts[0])\n",
    "            train_labels.append(desc_parts[1].split(\"-\")[0])\n",
    "\n",
    "    test_sequences = []\n",
    "    test_ids = []\n",
    "    test_labels = []\n",
    "    with open(\"../tmp/data/unprocessed/localization/new_hard_test_set_PIDE20.fasta_rep_seq.fasta\", \"r\") as f:\n",
    "        for record in SeqIO.parse(f, \"fasta\"):\n",
    "            test_sequences.append(str(record.seq))\n",
    "            desc_parts = record.description.split()\n",
    "            test_ids.append(desc_parts[0])\n",
    "            test_labels.append(desc_parts[1].split(\"-\")[0])\n",
    "\n",
    "    train_dataset = Dataset.from_dict({\"sequence\": train_sequences, \"id\": train_ids, \"location\": train_labels})\n",
    "\n",
    "    test_dataset = Dataset.from_dict({\"sequence\": test_sequences, \"id\": test_ids, \"location\": test_labels})\n",
    "\n",
    "    dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "\n",
    "    def process_sequences(sequences):\n",
    "        processed_sequences = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", seq))) for seq in sequences]\n",
    "        return processed_sequences\n",
    "\n",
    "    train_processed = process_sequences(dataset[\"train\"][\"sequence\"])\n",
    "    test_processed = process_sequences(dataset[\"test\"][\"sequence\"])\n",
    "\n",
    "    train_encodings = tokenizer(train_processed, padding=False, truncation=False)\n",
    "\n",
    "    test_encodings = tokenizer(test_processed, padding=False, truncation=False)\n",
    "\n",
    "    dataset[\"train\"] = dataset[\"train\"].add_column(\"processed_sequence\", train_processed)\n",
    "    dataset[\"train\"] = dataset[\"train\"].add_column(\"input_ids\", train_encodings[\"input_ids\"])\n",
    "    dataset[\"train\"] = dataset[\"train\"].add_column(\"attention_mask\", train_encodings[\"attention_mask\"])\n",
    "\n",
    "    dataset[\"test\"] = dataset[\"test\"].add_column(\"processed_sequence\", test_processed)\n",
    "    dataset[\"test\"] = dataset[\"test\"].add_column(\"input_ids\", test_encodings[\"input_ids\"])\n",
    "    dataset[\"test\"] = dataset[\"test\"].add_column(\"attention_mask\", test_encodings[\"attention_mask\"])\n",
    "\n",
    "    dataset[\"train\"] = dataset[\"train\"].map(lambda x: {\"labels\": label2location[x[\"location\"]]})\n",
    "    dataset[\"test\"] = dataset[\"test\"].map(lambda x: {\"labels\": label2location[x[\"location\"]]})\n",
    "\n",
    "    dataset.save_to_disk(\"../tmp/data/unprocessed/localization/deeploc_data_processed\")\n",
    "else:\n",
    "    dataset = load_from_disk(\"../tmp/data/unprocessed/localization/deeploc_data_processed\")\n",
    "\n",
    "\n",
    "print(f\"Dataset size in MB: {sum(dataset[split].data.nbytes for split in dataset) / (1024 * 1024):.2f}\")\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])\n",
    "print(dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lengths = [len(seq) for seq in dataset[\"train\"][\"sequence\"]]\n",
    "test_lengths = [len(seq) for seq in dataset[\"test\"][\"sequence\"]]\n",
    "\n",
    "max_train_length = max(train_lengths)\n",
    "max_test_length = max(test_lengths)\n",
    "max_length = max(max_train_length, max_test_length)\n",
    "\n",
    "print(f\"Longest sequence in train set: {max_train_length}\")\n",
    "print(f\"Longest sequence in test set: {max_test_length}\")\n",
    "print(f\"Longest sequence overall: {max_length}\")\n",
    "\n",
    "longest_idx = train_lengths.index(max_train_length) if max_train_length == max_length else test_lengths.index(max_test_length)\n",
    "longest_split = \"train\" if max_train_length == max_length else \"test\"\n",
    "print(f\"\\nExample of longest sequence (from {longest_split} set):\")\n",
    "print(dataset[longest_split][longest_idx][\"sequence\"])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.hist(train_lengths, bins=50, alpha=0.5, label=\"Train\", density=True)\n",
    "plt.hist(test_lengths, bins=50, alpha=0.5, label=\"Test\", density=True)\n",
    "\n",
    "plt.xlabel(\"Sequence Length\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Sequence Lengths\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.axvline(\n",
    "    sum(train_lengths) / len(train_lengths),\n",
    "    color=\"blue\",\n",
    "    linestyle=\"dashed\",\n",
    "    alpha=0.5,\n",
    "    label=f\"Train Mean: {sum(train_lengths)/len(train_lengths):.0f}\",\n",
    ")\n",
    "plt.axvline(\n",
    "    sum(test_lengths) / len(test_lengths),\n",
    "    color=\"orange\",\n",
    "    linestyle=\"dashed\",\n",
    "    alpha=0.5,\n",
    "    label=f\"Test Mean: {sum(test_lengths)/len(test_lengths):.0f}\",\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSequence length statistics:\")\n",
    "print(f\"Train set - Mean: {sum(train_lengths)/len(train_lengths):.1f}, Median: {sorted(train_lengths)[len(train_lengths)//2]}\")\n",
    "print(f\"Test set  - Mean: {sum(test_lengths)/len(test_lengths):.1f}, Median: {sorted(test_lengths)[len(test_lengths)//2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_allowed_length = 1024\n",
    "\n",
    "train_indices = [i for i, length in enumerate(train_lengths) if length <= max_allowed_length]\n",
    "dataset[\"train\"] = dataset[\"train\"].select(train_indices)\n",
    "\n",
    "test_indices = [i for i, length in enumerate(test_lengths) if length <= max_allowed_length]\n",
    "dataset[\"test\"] = dataset[\"test\"].select(test_indices)\n",
    "\n",
    "print(f\"Filtered dataset statistics:\")\n",
    "print(f\"Train set: {len(dataset['train'])} sequences (removed {len(train_lengths) - len(dataset['train'])} sequences)\")\n",
    "print(f\"Test set: {len(dataset['test'])} sequences (removed {len(test_lengths) - len(dataset['test'])} sequences)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sequence = dataset[\"train\"][0][\"input_ids\"]\n",
    "decoded_sequence = tokenizer.decode(example_sequence)\n",
    "print(\"Original sequence:\\t\\t\", dataset[\"train\"][0][\"sequence\"])\n",
    "print(\"Tokenized and decoded sequence:\\t\", decoded_sequence.replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nTrain set label distribution:\")\n",
    "# print(pd.Series(dataset[\"train\"][\"label\"]).value_counts())\n",
    "\n",
    "# print(\"\\nTest set label distribution:\")\n",
    "# print(pd.Series(dataset[\"test\"][\"label\"]).value_counts())\n",
    "\n",
    "# print(\"\\nNumber of distinct labels:\")\n",
    "# print(f\"Train: {len(set(dataset['train']['label']))}\")\n",
    "# print(f\"Test: {len(set(dataset['test']['label']))}\")\n",
    "\n",
    "\n",
    "# unique_labels = sorted(set(dataset[\"train\"][\"location\"]).union(set(dataset[\"test\"][\"location\"])))\n",
    "# label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "# id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# print(label2id)\n",
    "# print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_attention_mask(attention_mask, trim_beginning=0, trim_end=0):\n",
    "    \"\"\"\n",
    "    Finds indices of first n and last m 1s in attention mask and sets them to 0.\n",
    "    Vectorized implementation.\n",
    "    Args:\n",
    "        attention_mask: tensor of shape (batch_size, seq_length)\n",
    "        trim_beginning: number of 1s to trim from beginning\n",
    "        trim_end: number of 1s to trim from end\n",
    "    Returns:\n",
    "        Modified attention mask with first n and last m 1s set to 0\n",
    "    \"\"\"\n",
    "    if trim_beginning == 0 and trim_end == 0:\n",
    "        return attention_mask\n",
    "\n",
    "    attention_mask = attention_mask.clone()\n",
    "\n",
    "    cumsum_forward = torch.cumsum(attention_mask, dim=1)\n",
    "\n",
    "    cumsum_backward = torch.cumsum(attention_mask.flip(dims=[1]), dim=1).flip(dims=[1])\n",
    "\n",
    "    if trim_beginning > 0:\n",
    "        beginning_mask = cumsum_forward > trim_beginning\n",
    "        attention_mask = attention_mask * beginning_mask\n",
    "\n",
    "    if trim_end > 0:\n",
    "        end_mask = cumsum_backward > trim_end\n",
    "        attention_mask = attention_mask * end_mask\n",
    "\n",
    "    return attention_mask\n",
    "\n",
    "\n",
    "# From Hannes\n",
    "class LightAttention(nn.Module):\n",
    "    def __init__(self, embeddings_dim=1024, output_dim=11, dropout=0.25, kernel_size=9, conv_dropout: float = 0.25):\n",
    "        super(LightAttention, self).__init__()\n",
    "\n",
    "        self.feature_convolution = nn.Conv1d(embeddings_dim, embeddings_dim, kernel_size, stride=1, padding=kernel_size // 2)\n",
    "        self.attention_convolution = nn.Conv1d(embeddings_dim, embeddings_dim, kernel_size, stride=1, padding=kernel_size // 2)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.dropout = nn.Dropout(conv_dropout)\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(2 * embeddings_dim, 32), nn.Dropout(dropout), nn.ReLU(), nn.BatchNorm1d(32))\n",
    "\n",
    "        self.output = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, embeddings_dim, sequence_length] embedding tensor that should be classified\n",
    "            mask: [batch_size, sequence_length] mask corresponding to the zero padding used for the shorter sequecnes in the batch. All values corresponding to padding are False and the rest is True.\n",
    "\n",
    "        Returns:\n",
    "            classification: [batch_size,output_dim] tensor with logits\n",
    "        \"\"\"\n",
    "        o = self.feature_convolution(x)  # [batch_size, embeddings_dim, sequence_length]\n",
    "        o = self.dropout(o)  # [batch_gsize, embeddings_dim, sequence_length]\n",
    "        attention = self.attention_convolution(x)  # [batch_size, embeddings_dim, sequence_length]\n",
    "\n",
    "        attention = attention.masked_fill(mask[:, None, :] == False, -1e9)\n",
    "\n",
    "        o1 = torch.sum(o * self.softmax(attention), dim=-1)  # [batchsize, embeddings_dim]\n",
    "        o2, _ = torch.max(o, dim=-1)  # [batchsize, embeddings_dim]\n",
    "        o = torch.cat([o1, o2], dim=-1)  # [batchsize, 2*embeddings_dim]\n",
    "        o = self.linear(o)  # [batchsize, 32]\n",
    "        return self.output(o)  # [batchsize, output_dim]\n",
    "\n",
    "\n",
    "class LightAttentionPLM(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.light_attention = LightAttention(\n",
    "            output_dim=config.light_attention[\"output_dim\"],\n",
    "            dropout=config.light_attention[\"dropout\"],\n",
    "            kernel_size=config.light_attention[\"kernel_size\"],\n",
    "        )\n",
    "        self.light_attention.to(config.device)\n",
    "\n",
    "        self.plm = T5EncoderModel.from_pretrained(\n",
    "            pretrained_model_name_or_path=config.plm,\n",
    "            device_map=config.device,\n",
    "            torch_dtype=\"auto\",\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None, **kwargs):\n",
    "        x = self.plm(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = x.last_hidden_state[:, :1, :]\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        attention_mask = trim_attention_mask(attention_mask, trim_beginning=0, trim_end=1)\n",
    "        x = self.light_attention(x=x, mask=attention_mask)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(x.view(-1, x.size(-1)), labels.view(-1))\n",
    "\n",
    "        return modeling_outputs.SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=x,\n",
    "            hidden_states=None,\n",
    "            attentions=None,\n",
    "        )\n",
    "\n",
    "    def print_trainable_parameters(self):\n",
    "        \"\"\"\n",
    "        Prints the number of trainable parameters in the model.\n",
    "        \"\"\"\n",
    "        trainable_params = 0\n",
    "        all_param = 0\n",
    "        for _, param in self.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "        print(\n",
    "            f\"trainable params: {trainable_params:,d} || all params: {all_param:,d} \"\n",
    "            f\"|| trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
    "        )\n",
    "        \n",
    "# class LightAttentionCLIP(PreTrainedModel):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "\n",
    "#         self.light_attention = LightAttention(\n",
    "#             output_dim=config.light_attention[\"output_dim\"],\n",
    "#             dropout=config.light_attention[\"dropout\"],\n",
    "#             kernel_size=config.light_attention[\"kernel_size\"],\n",
    "#         )\n",
    "#         self.light_attention.to(config.device)\n",
    "\n",
    "#         self.clip = CLIPModel.from_pretrained(\n",
    "#             pretrained_model_name_or_path=config.plm,\n",
    "#             device_map=config.device,\n",
    "#             torch_dtype=\"auto\",\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask, labels=None, **kwargs):\n",
    "#         x = self.plm(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         x = x.last_hidden_state[:, :1, :]\n",
    "#         x = x.transpose(1, 2)\n",
    "\n",
    "#         attention_mask = trim_attention_mask(attention_mask, trim_beginning=0, trim_end=1)\n",
    "#         x = self.light_attention(x=x, mask=attention_mask)\n",
    "\n",
    "#         if labels is not None:\n",
    "#             loss_fct = nn.CrossEntropyLoss()\n",
    "#             loss = loss_fct(x.view(-1, x.size(-1)), labels.view(-1))\n",
    "\n",
    "#         return modeling_outputs.SequenceClassifierOutput(\n",
    "#             loss=loss,\n",
    "#             logits=x,\n",
    "#             hidden_states=None,\n",
    "#             attentions=None,\n",
    "#         )\n",
    "\n",
    "#     def print_trainable_parameters(self):\n",
    "#         \"\"\"\n",
    "#         Prints the number of trainable parameters in the model.\n",
    "#         \"\"\"\n",
    "#         trainable_params = 0\n",
    "#         all_param = 0\n",
    "#         for _, param in self.named_parameters():\n",
    "#             all_param += param.numel()\n",
    "#             if param.requires_grad:\n",
    "#                 trainable_params += param.numel()\n",
    "#         print(\n",
    "#             f\"trainable params: {trainable_params:,d} || all params: {all_param:,d} \"\n",
    "#             f\"|| trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
    "#         )\n",
    "\n",
    "\n",
    "\n",
    "class LinearPLM(PreTrainedModel):\n",
    "    pass\n",
    "\n",
    "\n",
    "class LinearCLIP(PreTrainedModel):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PretrainedConfig(\n",
    "    plm=train_config[\"model\"][\"plm\"],\n",
    "    output_hidden_states=True,\n",
    "    output_attentions=True,\n",
    "    return_dict=True,\n",
    "    device=device,\n",
    "    freeze_plm=train_config[\"model\"][\"freeze_plm\"],\n",
    "    light_attention=train_config[\"light_attention\"],\n",
    ")\n",
    "\n",
    "model = LightAttentionPLM(config)\n",
    "model = model.to(device)\n",
    "\n",
    "if train_config[\"model\"][\"freeze_plm\"]:\n",
    "    for param in model.plm.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if train_config[\"lora\"][\"enabled\"]:\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        inference_mode=False,\n",
    "        r=train_config[\"lora\"][\"r\"],\n",
    "        lora_alpha=train_config[\"lora\"][\"lora_alpha\"],\n",
    "        lora_dropout=train_config[\"lora\"][\"lora_dropout\"],\n",
    "        target_modules=train_config[\"lora\"][\"target_modules\"],\n",
    "        bias=\"none\",\n",
    "        modules_to_save=train_config[\"lora\"][\"modules_to_save\"],\n",
    "        use_rslora=train_config[\"lora\"][\"use_rslora\"],\n",
    "        use_dora=train_config[\"lora\"][\"use_dora\"],\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(\"target_modules:\", lora_config.target_modules)\n",
    "    print(\"modules_to_save:\", lora_config.modules_to_save)\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if not param.is_cuda:\n",
    "        print(f\"Warning: Parameter {name} is not on CUDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "split = \"test\"\n",
    "\n",
    "random_idx = random.randint(0, len(dataset[split]))\n",
    "with torch.no_grad():\n",
    "    out = model(\n",
    "        input_ids=torch.tensor([dataset[split][random_idx][\"input_ids\"]]).to(device),\n",
    "        attention_mask=torch.tensor([dataset[split][random_idx][\"attention_mask\"]]).to(device),\n",
    "        labels=torch.tensor([label2location[dataset[split][random_idx][\"location\"]]]).to(device),\n",
    "    )\n",
    "    \n",
    "print(dataset[split][random_idx][\"location\"])\n",
    "print(dataset[split][random_idx][\"sequence\"])\n",
    "print()\n",
    "print(out.logits.shape)\n",
    "print(out.logits.softmax(dim=-1))\n",
    "print(out.logits.argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightAttentionTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.eval_sample_size = kwargs.pop(\"eval_sample_size\", 32)\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def get_eval_dataloader(self, eval_dataset=None):\n",
    "        \"\"\"\n",
    "        Samples the evaluation dataset and returns a subset of size self.eval_sample_size.\n",
    "        \"\"\"\n",
    "        if eval_dataset is None and self.eval_dataset is None:\n",
    "            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
    "\n",
    "        # If we have persistent workers, don't do a fork bomb especially as eval datasets\n",
    "        # don't change during training\n",
    "        dataloader_key = eval_dataset if isinstance(eval_dataset, str) else \"eval\"\n",
    "        if (\n",
    "            hasattr(self, \"_eval_dataloaders\")\n",
    "            and dataloader_key in self._eval_dataloaders\n",
    "            and self.args.dataloader_persistent_workers\n",
    "        ):\n",
    "            return self.accelerator.prepare(self._eval_dataloaders[dataloader_key])\n",
    "\n",
    "        # Use random subset of eval dataset\n",
    "        eval_dataset = (\n",
    "            self.eval_dataset[eval_dataset]\n",
    "            if isinstance(eval_dataset, str)\n",
    "            else eval_dataset\n",
    "            if eval_dataset is not None\n",
    "            else self.eval_dataset\n",
    "        ).select(random.sample(range(len(self.eval_dataset)), self.eval_sample_size))\n",
    "        data_collator = self.data_collator\n",
    "\n",
    "        if is_datasets_available() and isinstance(eval_dataset, datasets.Dataset):\n",
    "            eval_dataset = self._remove_unused_columns(eval_dataset, description=\"evaluation\")\n",
    "        else:\n",
    "            data_collator = self._get_collator_with_removed_columns(data_collator, description=\"evaluation\")\n",
    "\n",
    "        dataloader_params = {\n",
    "            \"batch_size\": self.args.eval_batch_size,\n",
    "            \"collate_fn\": data_collator,\n",
    "            \"num_workers\": self.args.dataloader_num_workers,\n",
    "            \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "            \"persistent_workers\": self.args.dataloader_persistent_workers,\n",
    "        }\n",
    "\n",
    "        if not isinstance(eval_dataset, torch.utils.data.IterableDataset):\n",
    "            dataloader_params[\"sampler\"] = self._get_eval_sampler(eval_dataset)\n",
    "            dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
    "            dataloader_params[\"prefetch_factor\"] = self.args.dataloader_prefetch_factor\n",
    "\n",
    "        # accelerator.free_memory() will destroy the references, so\n",
    "        # we need to store the non-prepared version\n",
    "        eval_dataloader = DataLoader(eval_dataset, **dataloader_params)\n",
    "        if self.args.dataloader_persistent_workers:\n",
    "            if hasattr(self, \"_eval_dataloaders\"):\n",
    "                self._eval_dataloaders[dataloader_key] = eval_dataloader\n",
    "            else:\n",
    "                self._eval_dataloaders = {dataloader_key: eval_dataloader}\n",
    "\n",
    "        return self.accelerator.prepare(eval_dataloader)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../tmp/models/checkpoints/{train_config['project_name']}\",\n",
    "    run_name=train_config[\"project_name\"] if train_config[\"weights_and_biases\"][\"enabled\"] else None,\n",
    "    report_to=train_config[\"weights_and_biases\"][\"report_to\"] if train_config[\"weights_and_biases\"][\"enabled\"] else None,\n",
    "    learning_rate=train_config[\"trainer\"][\"learning_rate\"],\n",
    "    per_device_train_batch_size=train_config[\"trainer\"][\"train_batch_size\"],\n",
    "    num_train_epochs=train_config[\"trainer\"][\"num_epochs\"],\n",
    "    eval_strategy=train_config[\"trainer\"][\"eval_strategy\"],\n",
    "    eval_on_start=True,\n",
    "    eval_steps=train_config[\"trainer\"][\"eval_steps\"],\n",
    "    per_device_eval_batch_size=train_config[\"trainer\"][\"eval_batch_size\"],\n",
    "    save_strategy=train_config[\"trainer\"][\"save_strategy\"],\n",
    "    save_steps=train_config[\"trainer\"][\"save_steps\"],\n",
    "    save_total_limit=train_config[\"trainer\"][\"save_total_limit\"],\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=train_config[\"trainer\"][\"logging_steps\"],\n",
    "    seed=train_config[\"seed\"],\n",
    "    lr_scheduler_type=train_config[\"trainer\"][\"lr_scheduler_type\"],\n",
    "    warmup_steps=train_config[\"trainer\"][\"warmup_steps\"],\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True, pad_to_multiple_of=8)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    print(predictions)\n",
    "    print(labels)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\"),\n",
    "        \"recall\": recall_score(labels, predictions, average=\"weighted\"),\n",
    "        \"mcc\": matthews_corrcoef(labels, predictions),\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = LightAttentionTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"train\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_sample_size=train_config[\"trainer\"][\"eval_sample_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = f\"../tmp/models/{model_name_identifier}\"\n",
    "trainer.save_model(model_save_path)\n",
    "\n",
    "history = pd.DataFrame(trainer.state.log_history)\n",
    "history.to_csv(f\"{model_save_path}/training_log.csv\", index=False)\n",
    "\n",
    "with open(f\"{model_save_path}/train_config.yaml\", \"w\") as f:\n",
    "    train_config[\"model\"][\"reload_from_checkpoint_path\"] = model_save_path\n",
    "    yaml.dump(train_config, f, sort_keys=False)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "metrics = ['eval_accuracy', 'eval_f1', 'eval_recall', 'eval_mcc']\n",
    "for metric in metrics:\n",
    "    ax1.plot(history[history[metric].notna()]['step'], \n",
    "             history[history[metric].notna()][metric],\n",
    "             marker='o', label=metric.replace('eval_', ''))\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Evaluation Metrics')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "train_loss = history[history['loss'].notna()]\n",
    "eval_loss = history[history['eval_loss'].notna()]\n",
    "ax2.plot(train_loss['step'], train_loss['loss'], \n",
    "         label='Training Loss', marker='o')\n",
    "ax2.plot(eval_loss['step'], eval_loss['eval_loss'],\n",
    "         label='Evaluation Loss', marker='o')\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Training and Evaluation Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "fig.savefig(f\"{model_save_path}/training_history.png\")\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"Model, config, and log saved to:\", model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
