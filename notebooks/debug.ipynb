{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "from src.model.basic_model import ProtT5CLIP\n",
    "from src.model.data_collator import DataCollatorForProtT5CLIP\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2492269427cb47d2b2e2e35238f7bf15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_cfg = {\n",
    "    \"base_model_plm\": \"Rostlab/prot_t5_xl_uniref50\",\n",
    "    \"freeze_plm\": False,\n",
    "    \"base_model_llm\": \"microsoft/Phi-3.5-mini-instruct\",\n",
    "    \"freeze_llm\": False,\n",
    "}\n",
    "\n",
    "model = ProtT5CLIP(model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,388,608 || all params: 5,037,609,984 || trainable%: 0.1665\n"
     ]
    }
   ],
   "source": [
    "target_modules = []\n",
    "modules_to_save = []\n",
    "if not model_cfg[\"freeze_plm\"]:\n",
    "    target_modules += [\"q\", \"k\", \"v\", \"o\"]\n",
    "    modules_to_save = model.loading_info_plm[\"missing_keys\"]\n",
    "if not model_cfg[\"freeze_llm\"]:\n",
    "    target_modules += [\"k_proj\", \"q_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    "    modules_to_save += model.loading_info_llm[\"missing_keys\"]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=target_modules,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=modules_to_save,\n",
    "    # use_rslora=True,\n",
    "    # use_dora=True,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_plm = T5Tokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_cfg[\"base_model_plm\"],\n",
    "    do_lower_case=False,\n",
    "    use_fast=True,\n",
    "    legacy=False,\n",
    ")\n",
    "\n",
    "tokenizer_llm = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_cfg[\"base_model_llm\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ae6dfaddec47fda9fd0c0ab227b2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "})\n",
      "{'input_ids': {'sequence': [19, 4, 9, 6, 13, 6, 21, 12, 13, 12, 4, 3, 15, 3, 6, 5, 4, 5, 4, 5, 4, 4, 12, 13, 20, 4, 16, 14, 13, 15, 16, 8, 15, 13, 20, 4, 16, 14, 13, 15, 16, 8, 15, 1], 'text': [910, 26823, 338, 9701, 297, 3813, 10800, 8608, 29889]}, 'attention_mask': {'sequence': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'text': [1, 1, 1, 1, 1, 1, 1, 1, 1]}}\n"
     ]
    }
   ],
   "source": [
    "dataset = [\n",
    "    {\n",
    "        \"uid\": \"A001\",\n",
    "        \"sequence\": \"MLEVPVWIPILAFAVGLGLGLLIPHLQKPFQRFPHLQKPFQRF\",\n",
    "        \"text\": \"This protein is involved in membrane transport.\",\n",
    "    },\n",
    "    {\n",
    "        \"uid\": \"A002\",\n",
    "        \"sequence\": \"MSLEQKKGADIISKILQIQNSIGKTTSPSTLKTTSPSTLKT\",\n",
    "        \"text\": \"This enzyme catalyzes the hydrolysis of ATP.\",\n",
    "    },\n",
    "    {\n",
    "        \"uid\": \"A003\",\n",
    "        \"sequence\": \"MKMKQQGLVADLLPNIRVMKTFGHFVFNYYNDN\",\n",
    "        \"text\": \"This transcription factor regulates gene expression.\",\n",
    "    },\n",
    "] * 1000\n",
    "\n",
    "dataset = Dataset.from_list(dataset)\n",
    "dataset = dataset.add_column(\"sequence_original\", dataset[\"sequence\"])\n",
    "dataset = dataset.map(lambda x: {\"sequence\": \" \".join(list(re.sub(r\"[UZOB]\", \"X\", x[\"sequence\"])))})\n",
    "\n",
    "tknz_plm = tokenizer_plm(text=dataset[\"sequence\"], padding=False, truncation=False)\n",
    "tknz_llm = tokenizer_llm(text=dataset[\"text\"], padding=False, truncation=False)\n",
    "\n",
    "dataset = dataset.add_column(\n",
    "    \"input_ids\", [{\"sequence\": seq, \"text\": txt} for seq, txt in zip(tknz_plm[\"input_ids\"], tknz_llm[\"input_ids\"])]\n",
    ")\n",
    "dataset = dataset.add_column(\n",
    "    \"attention_mask\", [{\"sequence\": seq, \"text\": txt} for seq, txt in zip(tknz_plm[\"attention_mask\"], tknz_llm[\"attention_mask\"])]\n",
    ")\n",
    "\n",
    "dataset = dataset.remove_columns([\"uid\", \"sequence\", \"text\", \"sequence_original\"])\n",
    "dataset = DatasetDict({\"train\": dataset, \"test\": dataset})\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collating\n",
      "[{'sequence': [19, 4, 9, 6, 13, 6, 21, 12, 13, 12, 4, 3, 15, 3, 6, 5, 4, 5, 4, 5, 4, 4, 12, 13, 20, 4, 16, 14, 13, 15, 16, 8, 15, 13, 20, 4, 16, 14, 13, 15, 16, 8, 15, 1], 'text': [910, 26823, 338, 9701, 297, 3813, 10800, 8608, 29889]}, {'sequence': [19, 7, 4, 9, 16, 14, 14, 5, 3, 10, 12, 12, 7, 14, 12, 4, 16, 12, 16, 17, 7, 12, 5, 14, 11, 11, 7, 13, 7, 11, 4, 14, 11, 11, 7, 13, 7, 11, 4, 14, 11, 1], 'text': [910, 427, 14022, 29872, 17246, 29891, 10947, 278, 17546, 368, 29879, 275, 310, 27884, 29889]}, {'sequence': [19, 14, 19, 14, 16, 16, 5, 4, 6, 3, 10, 4, 4, 13, 17, 12, 8, 6, 19, 14, 11, 15, 5, 20, 15, 6, 15, 17, 18, 18, 17, 10, 17, 1], 'text': [910, 1301, 3395, 7329, 24378, 1078, 18530, 4603, 29889]}, {'sequence': [19, 4, 9, 6, 13, 6, 21, 12, 13, 12, 4, 3, 15, 3, 6, 5, 4, 5, 4, 5, 4, 4, 12, 13, 20, 4, 16, 14, 13, 15, 16, 8, 15, 13, 20, 4, 16, 14, 13, 15, 16, 8, 15, 1], 'text': [910, 26823, 338, 9701, 297, 3813, 10800, 8608, 29889]}, {'sequence': [19, 7, 4, 9, 16, 14, 14, 5, 3, 10, 12, 12, 7, 14, 12, 4, 16, 12, 16, 17, 7, 12, 5, 14, 11, 11, 7, 13, 7, 11, 4, 14, 11, 11, 7, 13, 7, 11, 4, 14, 11, 1], 'text': [910, 427, 14022, 29872, 17246, 29891, 10947, 278, 17546, 368, 29879, 275, 310, 27884, 29889]}, {'sequence': [19, 14, 19, 14, 16, 16, 5, 4, 6, 3, 10, 4, 4, 13, 17, 12, 8, 6, 19, 14, 11, 15, 5, 20, 15, 6, 15, 17, 18, 18, 17, 10, 17, 1], 'text': [910, 1301, 3395, 7329, 24378, 1078, 18530, 4603, 29889]}, {'sequence': [19, 4, 9, 6, 13, 6, 21, 12, 13, 12, 4, 3, 15, 3, 6, 5, 4, 5, 4, 5, 4, 4, 12, 13, 20, 4, 16, 14, 13, 15, 16, 8, 15, 13, 20, 4, 16, 14, 13, 15, 16, 8, 15, 1], 'text': [910, 26823, 338, 9701, 297, 3813, 10800, 8608, 29889]}, {'sequence': [19, 7, 4, 9, 16, 14, 14, 5, 3, 10, 12, 12, 7, 14, 12, 4, 16, 12, 16, 17, 7, 12, 5, 14, 11, 11, 7, 13, 7, 11, 4, 14, 11, 11, 7, 13, 7, 11, 4, 14, 11, 1], 'text': [910, 427, 14022, 29872, 17246, 29891, 10947, 278, 17546, 368, 29879, 275, 310, 27884, 29889]}, {'sequence': [19, 14, 19, 14, 16, 16, 5, 4, 6, 3, 10, 4, 4, 13, 17, 12, 8, 6, 19, 14, 11, 15, 5, 20, 15, 6, 15, 17, 18, 18, 17, 10, 17, 1], 'text': [910, 1301, 3395, 7329, 24378, 1078, 18530, 4603, 29889]}, {'sequence': [19, 4, 9, 6, 13, 6, 21, 12, 13, 12, 4, 3, 15, 3, 6, 5, 4, 5, 4, 5, 4, 4, 12, 13, 20, 4, 16, 14, 13, 15, 16, 8, 15, 13, 20, 4, 16, 14, 13, 15, 16, 8, 15, 1], 'text': [910, 26823, 338, 9701, 297, 3813, 10800, 8608, 29889]}, {'sequence': [19, 7, 4, 9, 16, 14, 14, 5, 3, 10, 12, 12, 7, 14, 12, 4, 16, 12, 16, 17, 7, 12, 5, 14, 11, 11, 7, 13, 7, 11, 4, 14, 11, 11, 7, 13, 7, 11, 4, 14, 11, 1], 'text': [910, 427, 14022, 29872, 17246, 29891, 10947, 278, 17546, 368, 29879, 275, 310, 27884, 29889]}, {'sequence': [19, 14, 19, 14, 16, 16, 5, 4, 6, 3, 10, 4, 4, 13, 17, 12, 8, 6, 19, 14, 11, 15, 5, 20, 15, 6, 15, 17, 18, 18, 17, 10, 17, 1], 'text': [910, 1301, 3395, 7329, 24378, 1078, 18530, 4603, 29889]}, {'sequence': [19, 4, 9, 6, 13, 6, 21, 12, 13, 12, 4, 3, 15, 3, 6, 5, 4, 5, 4, 5, 4, 4, 12, 13, 20, 4, 16, 14, 13, 15, 16, 8, 15, 13, 20, 4, 16, 14, 13, 15, 16, 8, 15, 1], 'text': [910, 26823, 338, 9701, 297, 3813, 10800, 8608, 29889]}, {'sequence': [19, 7, 4, 9, 16, 14, 14, 5, 3, 10, 12, 12, 7, 14, 12, 4, 16, 12, 16, 17, 7, 12, 5, 14, 11, 11, 7, 13, 7, 11, 4, 14, 11, 11, 7, 13, 7, 11, 4, 14, 11, 1], 'text': [910, 427, 14022, 29872, 17246, 29891, 10947, 278, 17546, 368, 29879, 275, 310, 27884, 29889]}, {'sequence': [19, 14, 19, 14, 16, 16, 5, 4, 6, 3, 10, 4, 4, 13, 17, 12, 8, 6, 19, 14, 11, 15, 5, 20, 15, 6, 15, 17, 18, 18, 17, 10, 17, 1], 'text': [910, 1301, 3395, 7329, 24378, 1078, 18530, 4603, 29889]}, {'sequence': [19, 4, 9, 6, 13, 6, 21, 12, 13, 12, 4, 3, 15, 3, 6, 5, 4, 5, 4, 5, 4, 4, 12, 13, 20, 4, 16, 14, 13, 15, 16, 8, 15, 13, 20, 4, 16, 14, 13, 15, 16, 8, 15, 1], 'text': [910, 26823, 338, 9701, 297, 3813, 10800, 8608, 29889]}, {'sequence': [19, 7, 4, 9, 16, 14, 14, 5, 3, 10, 12, 12, 7, 14, 12, 4, 16, 12, 16, 17, 7, 12, 5, 14, 11, 11, 7, 13, 7, 11, 4, 14, 11, 11, 7, 13, 7, 11, 4, 14, 11, 1], 'text': [910, 427, 14022, 29872, 17246, 29891, 10947, 278, 17546, 368, 29879, 275, 310, 27884, 29889]}, {'sequence': [19, 14, 19, 14, 16, 16, 5, 4, 6, 3, 10, 4, 4, 13, 17, 12, 8, 6, 19, 14, 11, 15, 5, 20, 15, 6, 15, 17, 18, 18, 17, 10, 17, 1], 'text': [910, 1301, 3395, 7329, 24378, 1078, 18530, 4603, 29889]}, {'sequence': [19, 4, 9, 6, 13, 6, 21, 12, 13, 12, 4, 3, 15, 3, 6, 5, 4, 5, 4, 5, 4, 4, 12, 13, 20, 4, 16, 14, 13, 15, 16, 8, 15, 13, 20, 4, 16, 14, 13, 15, 16, 8, 15, 1], 'text': [910, 26823, 338, 9701, 297, 3813, 10800, 8608, 29889]}, {'sequence': [19, 7, 4, 9, 16, 14, 14, 5, 3, 10, 12, 12, 7, 14, 12, 4, 16, 12, 16, 17, 7, 12, 5, 14, 11, 11, 7, 13, 7, 11, 4, 14, 11, 11, 7, 13, 7, 11, 4, 14, 11, 1], 'text': [910, 427, 14022, 29872, 17246, 29891, 10947, 278, 17546, 368, 29879, 275, 310, 27884, 29889]}]\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForProtT5CLIP(\n",
    "    tokenizer_plm=tokenizer_plm,\n",
    "    tokenizer_llm=tokenizer_llm,\n",
    "    padding=True,\n",
    "    pad_to_multiple_of=8,\n",
    ")\n",
    "\n",
    "collated_data = data_collator(dataset['train'].select(range(20)))\n",
    "collated_data#['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be621440b47942e8aeb3a42c577a4433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collating\n",
      "[{'input_ids': {'sequence': [19, 4, 9, 6, 13, 6, 21, 12, 13, 12, 4, 3, 15, 3, 6, 5, 4, 5, 4, 5, 4, 4, 12, 13, 20, 4, 16, 14, 13, 15, 16, 8, 15, 13, 20, 4, 16, 14, 13, 15, 16, 8, 15, 1], 'text': [910, 26823, 338, 9701, 297, 3813, 10800, 8608, 29889]}, 'attention_mask': {'sequence': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'text': [1, 1, 1, 1, 1, 1, 1, 1, 1]}}, {'input_ids': {'sequence': [19, 14, 19, 14, 16, 16, 5, 4, 6, 3, 10, 4, 4, 13, 17, 12, 8, 6, 19, 14, 11, 15, 5, 20, 15, 6, 15, 17, 18, 18, 17, 10, 17, 1], 'text': [910, 1301, 3395, 7329, 24378, 1078, 18530, 4603, 29889]}, 'attention_mask': {'sequence': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'text': [1, 1, 1, 1, 1, 1, 1, 1, 1]}}, {'input_ids': {'sequence': [19, 7, 4, 9, 16, 14, 14, 5, 3, 10, 12, 12, 7, 14, 12, 4, 16, 12, 16, 17, 7, 12, 5, 14, 11, 11, 7, 13, 7, 11, 4, 14, 11, 11, 7, 13, 7, 11, 4, 14, 11, 1], 'text': [910, 427, 14022, 29872, 17246, 29891, 10947, 278, 17546, 368, 29879, 275, 310, 27884, 29889]}, 'attention_mask': {'sequence': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'text': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}}]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sequence'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     45\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 47\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/gits/prostT5-CLIP/.venv/lib/python3.12/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/gits/prostT5-CLIP/.venv/lib/python3.12/site-packages/transformers/trainer.py:2236\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2233\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2236\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   2239\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/Developer/gits/prostT5-CLIP/.venv/lib/python3.12/site-packages/accelerate/data_loader.py:550\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/gits/prostT5-CLIP/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Developer/gits/prostT5-CLIP/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Developer/gits/prostT5-CLIP/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/gits/prostT5-CLIP/src/model/data_collator.py:27\u001b[0m, in \u001b[0;36mDataCollatorForProtT5CLIP.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     25\u001b[0m features_llm \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sequence, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mfeature\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msequence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m     28\u001b[0m         features_plm\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: sequence})\n\u001b[1;32m     29\u001b[0m         features_llm\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: text})\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sequence'"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../tmp/models/\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=1,\n",
    "    do_train=True,\n",
    "    do_eval=False,\n",
    "    evaluation_strategy=\"steps\",  # use eval_strategy\n",
    "    eval_steps=300,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=300,\n",
    "    remove_unused_columns=True,\n",
    "    # label_names=[\"labels\"],\n",
    "    seed=69420,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    return {\n",
    "        \"loss\": 1.0,\n",
    "        \"accuracy\": 0.5,\n",
    "        \"precision\": 0.5,\n",
    "        \"recall\": 0.5,\n",
    "        \"f1\": 0.5,\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    # eval_dataset=dataset['valid'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
