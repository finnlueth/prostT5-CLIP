{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathias/.conda/envs/protclip/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:\t cuda:0\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from _header import *\n",
    "\n",
    "from open_clip.loss import ClipLoss, SigLipLoss\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from pytorch_lightning import LightningModule\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find metric to optimize\n",
    "# hydra\n",
    "# dataloader +: tokenizer padding False, do padding in dataloader\n",
    "# do plots\n",
    "# implement GO-terms metric into val/test step\n",
    "\n",
    "# implement lora\n",
    "# cosine / lamdba scheduler\n",
    "# add test_step in Lightning TrainingModule \n",
    "# add batch enums to header\n",
    "# normalize during forward of protclip\n",
    "# add mlp / linear projection layer\n",
    "# implement test step for lightning train module\n",
    "# sigliploss logit_bias logit_scale\n",
    "# coca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Models and apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This protein is involved in membrane transport.\n",
      "MLEVPVWIPILAFAVGLGLGLLIPHLQKPFQRF\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {\"uid\": \"A001\", \"seq\": \"MLEVPVWIPILAFAVGLGLGLLIPHLQKPFQRF\", \"text\": \"This protein is involved in membrane transport.\"},\n",
    "    {\"uid\": \"A002\", \"seq\": \"MSLEQKKGADIISKILQIQNSIGKTTSPSTLKT\", \"text\": \"This enzyme catalyzes the hydrolysis of ATP.\"},\n",
    "    {\"uid\": \"A003\", \"seq\": \"MKMKQQGLVADLLPNIRVMKTFGHFVFNYYNDN\", \"text\": \"This transcription factor regulates gene expression.\"}\n",
    "]\n",
    "\n",
    "ex1_text = data[0][\"text\"]\n",
    "ex2_seq = data[0][\"seq\"]\n",
    "print(ex1_text)\n",
    "print(ex2_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLMEncoder(nn.Module):\n",
    "    \"\"\"wraps PLM encoders\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: Literal[\"protT5, prostT5, esm2\"] | str,\n",
    "        device: torch.device,\n",
    "        lora: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if model_name == \"protT5\":\n",
    "            self.mod_type = \"pt\"\n",
    "            self.model = T5EncoderModel.from_pretrained(\n",
    "            BASE_MODEL_PLM,\n",
    "            device_map=device,\n",
    "            torch_dtype='auto',\n",
    "            cache_dir=\"/mnt/volume/mathias/pretrained_models\"\n",
    "            )\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(BASE_MODEL_PLM)\n",
    "            \n",
    "        if lora:\n",
    "            print(\"IF LORA DOES COOL THINGS\")\n",
    "            \n",
    "    def freeze(self):\n",
    "        \"\"\"Freeze model params\"\"\"\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def forward(self, x, emb_type):\n",
    "        inputs = self.tokenizer(\n",
    "            x,\n",
    "            return_tensors = \"pt\",\n",
    "            max_length=10_000,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        #outputs = self.model(**inputs).last_hidden_state.cpu()\n",
    "        outputs = self.model(**inputs).last_hidden_state\n",
    "        # if emb_type == \"per_res\":\n",
    "        #     if self.mod_type in (\"pt\", \"ank\"):\n",
    "        #         outputs = outputs[:-1, :]\n",
    "        #     elif self.mod_type == \"esm\":\n",
    "        #         output = np.squeeze(outputs, axis=0)[:-1, :]\n",
    "        #     return outputs\n",
    "        \n",
    "        if emb_type == \"per_prot\":\n",
    "            #return outputs.mean(axis=1).flatten()\n",
    "            return outputs.mean(axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"Input valid embedding type\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMEncoder(nn.Module):\n",
    "    \"\"\"wraps LLM encoders\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: Literal[\"phi3.5\"] | str,\n",
    "        device: torch.device,\n",
    "        lora: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        if model_name == \"phi3.5\":\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                BASE_MODEL_LLM,\n",
    "                device_map=device,\n",
    "                torch_dtype=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=\"/mnt/volume/mathias/pretrained_models\"\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_LLM)\n",
    "         \n",
    "        if lora:\n",
    "            print(\"IF LORA: DO COOL THINGS\")\n",
    "            \n",
    "        \n",
    "    def freeze(self):\n",
    "        \"\"\"Freeze model params\"\"\"\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x, sentence_level=True):\n",
    "        \"\"\"Forward pass, extract token or sentence embeddings\"\"\"\n",
    "        inputs = self.tokenizer(x, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        outputs = self.model(**inputs, output_hidden_states=True)\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        \n",
    "        if sentence_level:\n",
    "            # Average over tokens to get sentence embedding\n",
    "            embeddings = last_hidden_state.mean(dim=1)\n",
    "        else:\n",
    "            # Keep per-token embeddings\n",
    "            embeddings = last_hidden_state.squeeze(0)\n",
    "        \n",
    "        #return embeddings.detach().cpu().float().numpy()   # detach to numpy, remove detach() and numpy() if needed for further computation\n",
    "        return embeddings.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtCLIP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        plm_name: Literal[\"protT5\"],\n",
    "        llm_name: Literal[\"phi3.5\"],\n",
    "        loss: Literal[\"CLIP\", \"SIGLIP\"],\n",
    "        temperature: float,\n",
    "        device: torch.device,\n",
    "        lora: bool = False,\n",
    "        \n",
    "    ):\n",
    "        super().__init__()\n",
    "        # REMOVE FREEZE\n",
    "        self.plm_encoder = PLMEncoder(model_name=plm_name, device=device, lora=lora)\n",
    "        self.llm_encoder = LLMEncoder(model_name=llm_name, device=device, lora=lora)\n",
    "        if loss == \"CLIP\":  \n",
    "            self.loss = ClipLoss()\n",
    "            \n",
    "        self.temperature = temperature\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / temperature)) # temporary\n",
    "    \n",
    "    def forward(self, batch: dict[str, Tensor]):\n",
    "        prot_embed = self.plm_encoder(batch[\"seq\"], \"per_prot\")\n",
    "        txt_embed = self.llm_encoder(batch[\"text\"], sentence_level=True) # sentence level true correct?\n",
    "        # add normalization\n",
    "        return prot_embed, txt_embed\n",
    "    \n",
    "    def compute_loss(self, prot_embed, txt_embed):\n",
    "        return self.loss(prot_embed, txt_embed, logit_scale=self.logit_scale.exp()) # logit_scale temperature in init?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingModule(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        plm_name: Literal[\"protT5\"], \n",
    "        llm_name: Literal[\"phi3.5\"],\n",
    "        loss: Literal[\"CLIP\", \"SIGLIP\"],\n",
    "        temperature: float,\n",
    "        device: torch.device,\n",
    "        lora: bool,\n",
    "        lr: float,\n",
    "        weight_decay: float,\n",
    "        cfg: DictConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.save_hyperparameters(self.cfg)\n",
    "        self.model = ProtCLIP(plm_name=plm_name, llm_name=llm_name, loss=loss, temperature=temperature, device=device)\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # eval\n",
    "        self.eval_outputs = {\"val\": []}\n",
    "        # add more \n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        params = []\n",
    "        for p in self.model.parameters():\n",
    "            if p.requires_grad:\n",
    "                params.append(p)\n",
    "        optimizer = torch.optim.AdamW(params, lr=self.lr, weight_decay=self.weight_decay) # configure betas\n",
    "        # also add scheduler here\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, metrics = self.shared_step(batch, \"train\")\n",
    "        self.log_dict(metrics, sync_dist=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, metrics = self.shared_step(batch, \"val\")\n",
    "        self.log_dict(metrics,sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def shared_step(self, batch, set: str):\n",
    "        prot_embed, txt_embed = self.model(batch)\n",
    "        \n",
    "        print(f\"prot_embed_dim: {prot_embed.shape}\")\n",
    "        print(f\"txt_embed_dim: {txt_embed.shape}\")\n",
    "\n",
    "        loss = self.model.compute_loss(prot_embed.T, txt_embed.T)        \n",
    "        metrics = {f\"{set}_loss\": loss}\n",
    "        return loss, metrics\n",
    "    \n",
    "    # def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "    #     return self.model(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.43s/it]\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type     | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | model | ProtCLIP | 5.0 B  | train\n",
      "-------------------------------------------\n",
      "5.0 B     Trainable params\n",
      "0         Non-trainable params\n",
      "5.0 B     Total params\n",
      "20,116.886Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "862       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathias/.conda/envs/protclip/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prot_embed_dim: torch.Size([1, 1024])\n",
      "txt_embed_dim: torch.Size([1, 3072])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (3072) to match target batch_size (1024).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 83\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# add result plots\u001b[39;00m\n\u001b[1;32m     69\u001b[0m cfg \u001b[38;5;241m=\u001b[39m DictConfig(\n\u001b[1;32m     70\u001b[0m     {\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplm_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotT5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m     }\n\u001b[1;32m     81\u001b[0m )\n\u001b[0;32m---> 83\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 65\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# add Logger (wandb/tensorboard)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     53\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mepochs,\n\u001b[1;32m     54\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[38;5;66;03m# Mixed precision for faster training (optional)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/protclip/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/protclip/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/.conda/envs/protclip/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/protclip/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/protclip/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1023\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1025\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/.conda/envs/protclip/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1052\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1049\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1052\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1054\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/protclip/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/protclip/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/protclip/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/protclip/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    322\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/.conda/envs/protclip/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:411\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[50], line 41\u001b[0m, in \u001b[0;36mTrainingModule.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m---> 41\u001b[0m     loss, metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dict(metrics,sync_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "Cell \u001b[0;32mIn[50], line 51\u001b[0m, in \u001b[0;36mTrainingModule.shared_step\u001b[0;34m(self, batch, set)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprot_embed_dim: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprot_embed\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtxt_embed_dim: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtxt_embed\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprot_embed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtxt_embed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     52\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mset\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss}\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, metrics\n",
      "Cell \u001b[0;32mIn[27], line 29\u001b[0m, in \u001b[0;36mProtCLIP.compute_loss\u001b[0;34m(self, prot_embed, txt_embed)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, prot_embed, txt_embed):\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprot_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtxt_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogit_scale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/protclip/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/protclip/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/protclip/lib/python3.12/site-packages/open_clip/loss.py:128\u001b[0m, in \u001b[0;36mClipLoss.forward\u001b[0;34m(self, image_features, text_features, logit_scale, output_dict)\u001b[0m\n\u001b[1;32m    122\u001b[0m logits_per_image, logits_per_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_logits(image_features, text_features, logit_scale)\n\u001b[1;32m    124\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_ground_truth(device, logits_per_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    126\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    127\u001b[0m     F\u001b[38;5;241m.\u001b[39mcross_entropy(logits_per_image, labels) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m--> 128\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits_per_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m ) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontrastive_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: total_loss} \u001b[38;5;28;01mif\u001b[39;00m output_dict \u001b[38;5;28;01melse\u001b[39;00m total_loss\n",
      "File \u001b[0;32m~/.conda/envs/protclip/lib/python3.12/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (3072) to match target batch_size (1024)."
     ]
    }
   ],
   "source": [
    "\n",
    "class ExampleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prot = self.data[idx]\n",
    "        return prot\n",
    "\n",
    "def train(cfg: DictConfig):\n",
    "    data_train = [\n",
    "    {\"uid\": \"A001\", \"seq\": \"MLEVPVWIPILAFAVGLGLGLLIPHLQKPFQRF\", \"text\": \"This protein is involved in membrane transport.\"},\n",
    "    {\"uid\": \"A002\", \"seq\": \"MSLEQKKGADIISKILQIQNSIGKTTSPSTLKT\", \"text\": \"This enzyme catalyzes the hydrolysis of ATP.\"}\n",
    "    ]\n",
    "    data_val = [\n",
    "    {\"uid\": \"A003\", \"seq\": \"MKMKQQGLVADLLPNIRVMKTFGHFVFNYYNDN\", \"text\": \"This transcription factor regulates gene expression.\"}\n",
    "    ]\n",
    "    \n",
    "    train_set = ExampleDataset(data_train)\n",
    "    val_set = ExampleDataset(data_val)\n",
    "    train_loader = DataLoader(train_set, batch_size=cfg.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=cfg.batch_size, shuffle=False)\n",
    "    \n",
    "    model = TrainingModule(\n",
    "        plm_name=cfg.plm_name,\n",
    "        llm_name=cfg.llm_name,\n",
    "        loss=cfg.loss,\n",
    "        temperature=cfg.temperature,\n",
    "        device=device,\n",
    "        lora=cfg.lora,\n",
    "        lr=cfg.lr,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "        cfg=cfg\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        save_last=True,\n",
    "        save_top_k=2,\n",
    "        mode=\"min\",\n",
    "        dirpath=\"/mnt/volumne/mathias/checkpoints/\",\n",
    "        auto_insert_metric_name=True,\n",
    "    )\n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "    es = EarlyStopping(min_delta=1e-3, patience=5, mode=\"min\", monitor=\"val_loss\")\n",
    "    # add Logger (wandb/tensorboard)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        max_epochs=cfg.epochs,\n",
    "        accelerator=\"gpu\",\n",
    "        enable_progress_bar=True,\n",
    "        strategy=\"auto\",\n",
    "        precision=\"bf16-mixed\",\n",
    "        log_every_n_steps=1,\n",
    "        check_val_every_n_epoch=1,\n",
    "        gradient_clip_val=5,\n",
    "        callbacks=[checkpoint_callback, lr_monitor, es],\n",
    "        #logger=logger,\n",
    "          # Mixed precision for faster training (optional)\n",
    "    )\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    # add result plots\n",
    "    \n",
    "\n",
    "cfg = DictConfig(\n",
    "    {\n",
    "    \"plm_name\": \"protT5\",\n",
    "    \"llm_name\": \"phi3.5\",\n",
    "    \"loss\": \"CLIP\",\n",
    "    \"temperature\": 0.07,\n",
    "    \"batch_size\": 16,\n",
    "    \"lr\": 1e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"epochs\": 10,\n",
    "    \"lora\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "train(cfg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLMEncoder(\n",
      "  (model): T5EncoderModel(\n",
      "    (shared): Embedding(128, 1024)\n",
      "    (encoder): T5Stack(\n",
      "      (embed_tokens): Embedding(128, 1024)\n",
      "      (block): ModuleList(\n",
      "        (0): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (relative_attention_bias): Embedding(32, 32)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "                (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1-23): 23 x T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "                (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#llm1 = LLMEncoder(model_name=\"phi3.5\", device=device, lora=False)\n",
    "#out1 = llm1(ex1_text, True)\n",
    "\n",
    "plm1 = PLMEncoder(model_name=\"protT5\", device=device, lora=False)\n",
    "#out2 = plm1(ex2_seq, \"per_prot\")\n",
    "\n",
    "#pc1 = ProtCLIP(\"protT5\", \"phi3.5\", \"CLIP\", device, False)\n",
    "#out3 = pc1(data[0])\n",
    "\n",
    "#print(llm1)\n",
    "print(plm1)\n",
    "#print(out2)\n",
    "#print(out3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLM\n",
    "\n",
    "# plm_tokenizer = T5Tokenizer.from_pretrained(\n",
    "#     pretrained_model_name_or_path=BASE_MODEL_PLM,\n",
    "#     do_lower_case=False,\n",
    "#     use_fast=True,\n",
    "#     legacy=False,\n",
    "# )\n",
    "\n",
    "# plm_model, plm_loading_info = T5EncoderModel.from_pretrained(\n",
    "#     pretrained_model_name_or_path=BASE_MODEL_PLM,\n",
    "#     output_loading_info=True,\n",
    "#     device_map=device,\n",
    "#     # load_in_8bit=False,\n",
    "#     # custom_dropout_rate=0.1,\n",
    "# )\n",
    "\n",
    "plm_lora_config = LoraConfig(\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q\", \"k\", \"v\", \"o\"],\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "plm_model = peft.get_peft_model(plm_model, plm_lora_config)\n",
    "plm_model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "# LLM\n",
    "\n",
    "# llm_tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     pretrained_model_name_or_path=BASE_MODEL_LLM\n",
    "# )\n",
    "\n",
    "# llm_model, llm_loading_info = AutoModelForCausalLM.from_pretrained(\n",
    "#     BASE_MODEL_LLM,\n",
    "#     device_map=device,\n",
    "#     torch_dtype=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "#     output_loading_info=True,\n",
    "# )\n",
    "\n",
    "llm_lora_config = LoraConfig(\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "llm_model = peft.get_peft_model(llm_model, llm_lora_config)\n",
    "llm_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ClipLoss(nn.Module):\n",
    "#     def __init__(self, strategy: str):\n",
    "#         self.strategy = strategy\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, prot_features, txt_features, logit_scale):\n",
    "#         logits_prot = logit_scale * prot_features @ txt_features.T\n",
    "#         logits_txt = logit_scale * txt_features @ prot_features.T\n",
    "\n",
    "#         labels = torch.arange(logits_prot.shape[0], dtype=torch.long, device=logits_prot.device)\n",
    "\n",
    "#         cl_prot = F.cross_entropy(logits_prot, labels)\n",
    "#         cl_txt = F.cross_entropy(logits_txt, labels)\n",
    "\n",
    "#         total_loss = (cl_prot + cl_txt) / 2\n",
    "#         return total_loss.mean(), cl_prot.mean(), cl_txt.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"uid\": \"A001\", \"seq\": \"MLEVPVWIPILAFAVGLGLGLLIPHLQKPFQRF\", \"text\": \"This protein is involved in membrane transport.\"},\n",
    "    {\"uid\": \"A002\", \"seq\": \"MSLEQKKGADIISKILQIQNSIGKTTSPSTLKT\", \"text\": \"This enzyme catalyzes the hydrolysis of ATP.\"},\n",
    "    {\"uid\": \"A003\", \"seq\": \"MKMKQQGLVADLLPNIRVMKTFGHFVFNYYNDN\", \"text\": \"This transcription factor regulates gene expression.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    protein_features = plm_base_model()['last_hidden_state']\n",
    "    language_features = llm_base_model()['last_hidden_state']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
